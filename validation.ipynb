{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from portfolio_optimization_class import PortfolioOptimization, optimize_windows\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_quarters = 32\n",
    "tau = 0.05\n",
    "maximum_weight = 0.3\n",
    "data_path = r'data\\random_data\\n_stocks_per_sector.csv'\n",
    "portfolios = optimize_windows(number_of_quarters, tau, maximum_weight, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100000,23) (22,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-9d44300c1cff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mportfolios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"^w\\d+$\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mexpectile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mportfolios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"EVAR\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mweighted_returns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mvalidation_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_returns\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mexpectile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_returns\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mexpectile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100000,23) (22,) "
     ]
    }
   ],
   "source": [
    "copulas = [\"clayton_random\", \"gaussian\", \"t_student\"]\n",
    "distributions = [\"gauss_dist\", \"t_dist\"]\n",
    "\n",
    "for copula in copulas:\n",
    "    for window in range(0, 64):\n",
    "        for dist in distributions:\n",
    "            if window % 5:\n",
    "                print(f\"Copula: {copula}, Window: {window}, Distribution: {dist}\")\n",
    "            simulated_returns_file_path = r'../data_return_rates/simulated_' + copula + '_RETURNS_22_stocks' + f'_{window}_window_{dist}.csv'\n",
    "            returns = pd.read_csv(simulated_returns_file_path, index_col=0)\n",
    "\n",
    "            weights = portfolios.iloc[window].filter(regex=\"^w\\d+$\").values\n",
    "            expectile = -portfolios.iloc[window][\"EVAR\"]\n",
    "            weighted_returns = returns.values * weights\n",
    "\n",
    "            validation_values = (1 - tau) * np.minimum(weighted_returns - expectile, 0) - tau * np.maximum(weighted_returns - expectile, 0)\n",
    "            scoring_values = (1 - tau) * np.minimum((weighted_returns - expectile)**2, 0) + tau * np.maximum((weighted_returns - expectile)**2, 0)\n",
    "\n",
    "            expected_validation_values = np.mean(validation_values, axis=1)\n",
    "            expected_scoring_values = np.mean(scoring_values, axis=1)\n",
    "\n",
    "            validation_df = pd.DataFrame({\n",
    "                'Portfolio Return': np.sum(weighted_returns, axis=1),\n",
    "                'Expected Validation Value': expected_validation_values,\n",
    "                'Expected Scoring Value': expected_scoring_values\n",
    "            })\n",
    "            output_path = r\"../scores/\" + f'{copula}_22_stocks_{window}_window_{dist}_SCORES.csv'\n",
    "            validation_df.to_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolios.iloc[63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns_df = pd.read_csv(r'../data_return_rates/simulated_clayton_random_RETURNS_22_stocks_0_window_gauss_dist.csv', index_col=0)\n",
    "\n",
    "# weights = portfolios.iloc[0].filter(regex=\"^w\\d+$\").values\n",
    "# expectile = -portfolios.iloc[0][\"EVAR\"]\n",
    "\n",
    "\n",
    "# weighted_returns = returns_df.values * weights\n",
    "\n",
    "\n",
    "# validation_values = (1 - tau) * np.minimum(weighted_returns - expectile, 0) - tau * np.maximum(weighted_returns - expectile, 0)\n",
    "# scoring_values = (1 - tau) * np.minimum((weighted_returns - expectile)**2, 0) + tau * np.maximum((weighted_returns - expectile)**2, 0)\n",
    "\n",
    "\n",
    "# expected_validation_values = np.mean(validation_values, axis=1)\n",
    "# expected_scoring_values = np.mean(scoring_values, axis=1)\n",
    "\n",
    "\n",
    "# validation_df = pd.DataFrame({\n",
    "#     'Portfolio Return': np.sum(weighted_returns, axis=1),\n",
    "#     'Expected Validation Value': expected_validation_values,\n",
    "#     'Expected Scoring Value': expected_scoring_values\n",
    "# })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# for i in range(200):\n",
    "#     returns = random.sample(validation_df[\"Portfolio Return\"].to_list(), 12)\n",
    "#     total_return = 1\n",
    "#     for return_rate in returns:\n",
    "#         total_return = total_return * (1 + return_rate)\n",
    "#     print(total_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cplex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
